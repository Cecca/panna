---
title: "Extremal value theory for Maximum Inner Product Search"
---

```{python}
import numpy as np
import h5py
import seaborn as sns
import matplotlib.pyplot as plt
```

```{python}
with h5py.File('core/.glove-100-angular.hdf5') as hfp:
    glove = hfp['train'][:100001]
    queries = hfp['test'][:]
    glove = glove / np.linalg.norm(glove, axis=1)[:, np.newaxis]
    queries = queries / np.linalg.norm(queries, axis=1)[:, np.newaxis]


(n, d) = glove.shape
D = 1024
s = 10
```

```{python}
proj = np.random.normal(size=(d, D))
```

```{python}
projected = np.dot(glove, proj)
qprojected = np.dot(queries, proj)
```

Then we can look at the "profile" of the dataset, i.e. the maximum inner products along each dimension

```{python}
sns.lineplot( np.max(projected, axis=0) )
```

## For estimation

```{python}
q_id = 0
d_id = np.argmax(np.dot(glove, queries[q_id,:]))
ans = np.dot(glove[d_id, :], queries[q_id, :])
ans
```

Let's take the projections
```{python}
dproj = projected[d_id]
qproj = qprojected[q_id]

expectation = ans * np.sqrt(2 * np.log(D))
variance = (np.linalg.norm(glove[d_id, :]) - ans*ans) / (2*np.log(D))
expectation, variance
```

```{python}
plt.plot(qproj, dproj, 'o')
```

```{python}
# order statistics
oss = np.argsort( -qproj )

# the scaling term for the estimator
correction = np.sqrt(2*np.log(D))

# take the average of the first few largest (and smallest order statistics)
s = 10
est = 0
for i in range(s):
    ii, jj = oss[i], oss[len(oss)-i-1]
    print(ii, jj)
    print(dproj[ii], dproj[jj])
    est += dproj[ii] / correction
    est += -dproj[jj] / correction
est /= 2*s

est, ans
```


## Top-k MIPS

For the top-k MIPS case, we can look at the  followin gplot, which reports
numbers out of Equation 3 of Pham's paper, which states the probability that
two with inner product $\tau_1$ and $\tau_2$ with the query trade ranks in the
first order concomitant, considering $D$.

```{python}
tau1 = 0.9
tau2 = 0.1
sigma = np.sqrt(1 - tau1*tau1) + np.sqrt(1 - tau2*tau2)
Ds  = np.arange(1, 2**10)
def plot_probs(tau1, tau2, s=1):
    Prs = np.power(Ds, -s*(tau1 - tau2)/(sigma*sigma))
    plt.plot(Ds, Prs, label=f"{tau1} - {tau2}")

plot_probs(0.9, 0.1)
plot_probs(0.9, 0.5)
plot_probs(0.6, 0.1)
plot_probs(0.6, 0.5)
plt.legend()
plt.xscale("log", base=2)
plt.xlabel("D")
plt.show()
```

```{python}
def plot_probs(tau1, tau2, D=2**9):
    s = np.arange(1, 20)  
    Prs = np.power(D, -s*(tau1 - tau2)/(sigma*sigma))
    plt.plot(s, Prs, label=f"{tau1} - {tau2}")

plot_probs(0.9, 0.1)
plot_probs(0.9, 0.8)
plt.legend()
plt.xlabel("S")
#plt.yscale("log")
plt.show()
```




